\section{Experiments}
\label{sec:experiments}

To demonstrate \meta/'s effectiveness in three crucial fields, we evaluate its
performance in NLP, IR, and ML tasks. All experiments were performed on a
workstation with an Intel(R) Core(TM) i7-5820K CPU, 16 GB of RAM, and a 4
TB 5900 RPM disk.

\subsection{Natural Language Processing}

Talk about parsers and POS taggers.

Results in Table~\ref{table:nlp-pos}.
Results in Table~\ref{table:nlp-shallow}.

\input{nlp-datasets}
\input{nlp-pos}
\input{nlp-shallow}

\subsection{Information Retrieval}

\meta/'s IR performance is compared with two well-known search engine toolkits:
\textsc{Lucene}'s latest version 5.5.0\footnote{\url{http://lucene.apache.org/}} and
\textsc{Indri}'s version 5.9\footnote{Indri 5.10 does not provide source
    code packages and thus could not be used. It is also known as
\textsc{Lemur}.}~\cite{lemur}.

For the IR experiments, we use the TREC blog06~\cite{blog06} permalink documents
and TREC gov2 corpus~\cite{gov2}. To ensure a more uniform indexing environment,
all HTML is cleaned before indexing. In addition, each corpus is converted into
a single file with one document per line to reduce the effects of many file
operations.

During indexing, terms are lower-cased, stop words are removed from a common
list of 431 stop words, Porter2 (\meta/) or Porter (Indri, Lucene) stemming is
performed, a maximum word length of 32 characters is set, original documents are
not stored in the index, and term position information is not
stored\footnote{For Indri, we are unable to disable positions information
storage.}.

We compare the following: indexing speed (Table~\ref{table:ir-indexing}), index
size in Table~\ref{table:ir-index-size}, query speed in
Table~\ref{table:ir-query-speed}, and query accuracy with BM25 ($k_1=0.9,
b=0.4$) in Table~\ref{table:ir-map}. We use the standard TREC queries associated
with each dataset and score each system's search results with the usual
\texttt{trec\_eval} program\footnote{\url{http://trec.nist.gov/trec_eval/}}.

\meta/ leads in indexing speed, though we note that \meta/'s default
indexer is multithreaded and \textsc{Lucene} does not provide a parallel
one\footnote{Additionally, we did not feel that writing a correct and threadsafe
indexer as a user is something to be reasonably expected.}. \meta/ creates the
smallest index for gov2 while \textsc{Lucene} creates the smallest index for
blog06; \textsc{Indri} greatly lags behind both. \meta/ follows \textsc{Lucene}
closely in retrieval speed, with \textsc{Indri} again lagging. As expected,
query performance between the three systems is relatively even, and we attribute
any small difference in MAP or precision to idiosyncrasies during tokenization.

\input{ir-datasets}
\input{ir-indexing}
\input{ir-index-size}
\input{ir-query-speed}
\input{ir-map}

\subsection{Machine Learning}

\meta/'s ML performance is compared with \textsc{liblinear}~\cite{liblinear}, a
well-known SVM library; \textsc{scikit-learn}~\cite{scikit} a Python ML library;
and \textsc{SVMMulticlass}~\cite{svmmulticlass}, a competitor to
\textsc{liblinear}. We focus on linear classification across these tools.
Statistics for the five datasets used can be found in
Table~\ref{table:ml-datasets}.

Look at MALLET~\cite{mallet}.

Four of the datasets
(20news\footnote{\url{http://qwone.com/~jason/20Newsgroups/}}, Newegg, and the
Yelp Academic Dataset\footnote{\url{https://www.yelp.com/academic_dataset}}) are
textual datasets. The Newegg (crawled ourselves) and Yelp datasets are review
datasets, and we consider only a partial list of the whole dataset where we have
removed all three-star reviews. We used MeTA for tokenization and feature
generation, applying the same constraints as we did for the indexing tests.
Randomized training (two thirds) and test splits (one third) were generated for
each of these datasets. For rcv1, we used the existing tokenization and
training/test splits available on the LIBSVM data
website~\footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/}}.

In Table~\ref{table:ml-exp}, we can see that \meta/ performs well both in
terms of speed and accuracy, and presents itself as a viable option in the
machine learning domain.

\input{ml-datasets}
\input{ml-exp}
