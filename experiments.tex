\section{Experiments}
\label{sec:experiments}

To demonstrate \meta/'s effectiveness in three crucial fields, we evaluate its
performance in NLP, IR, and ML tasks. All experiments were performed on a
workstation with an Intel(R) Core(TM) i7-5820K CPU, 16 GB of RAM, and a 4
TB 5900 RPM disk.

\subsection{Natural Language Processing}

\todo{POS tagging, very short.}

\input{nlp-parsing}

Both \meta/ and CoreNLP provide implementations of shift-reduce
constituency parsers, following the framework of \newcite{const-parsing}.
These can be trained greedily or via beam search. We compared the parser
implementations in \meta/ and CoreNLP along two dimensions---speed,
measured in wall time, and memory consumption, measured as maximum resident
set size---for both training and testing a greedy and beam search parser
(with a beam size of 4). Training was performed on the standard training
split (sections 2--21) of the Penn Treebank, with section 22 being used as
a development set (only used by CoreNLP). Section 23 was held out for
evaluation. The results are summarized in Table~\ref{table:nlp-parsing}.

\meta/ consistently uses less RAM than CoreNLP, both at training
time and testing time. Its training time is slower than CoreNLP
for the greedy parser, but less than half of CoreNLP's training time for
the beam parser. \meta/'s beam parser has worse labeled $F_1$ score, likely
the result of its simpler model averaging strategy\footnote{At training
time, both CoreNLP and \meta/ perform model averaging, but \meta/
computes the average over all updates and CoreNLP performs
cross-validation over a default of the best 8 models on the development
set.}. Overall, however, \meta/'s shift-reduce parser is competitive and
particularly lightweight.

\subsection{Information Retrieval}

\meta/'s IR performance is compared with two well-known search engine toolkits:
\textsc{Lucene}'s latest version 5.5.0\footnote{\url{http://lucene.apache.org/}} and
\textsc{Indri}'s version 5.9\footnote{Indri 5.10 does not provide source
    code packages and thus could not be used. It is also known as
\textsc{Lemur}.}~\cite{lemur}.

For the IR experiments, we use the TREC blog06~\cite{blog06} permalink documents
and TREC gov2 corpus~\cite{gov2}. To ensure a more uniform indexing environment,
all HTML is cleaned before indexing. In addition, each corpus is converted into
a single file with one document per line to reduce the effects of many file
operations.

During indexing, terms are lower-cased, stop words are removed from a common
list of 431 stop words, Porter2 (\meta/) or Porter (Indri, Lucene) stemming is
performed, a maximum word length of 32 characters is set, original documents are
not stored in the index, and term position information is not
stored\footnote{For Indri, we are unable to disable positions information
storage.}.

We compare the following: indexing speed (Table~\ref{table:ir-indexing}), index
size in Table~\ref{table:ir-index-size}, query speed in
Table~\ref{table:ir-query-speed}, and query accuracy with BM25 ($k_1=0.9,
b=0.4$) in Table~\ref{table:ir-map}. We use the standard TREC queries associated
with each dataset and score each system's search results with the usual
\texttt{trec\_eval} program\footnote{\url{http://trec.nist.gov/trec_eval/}}.

\meta/ leads in indexing speed, though we note that \meta/'s default
indexer is multithreaded and \textsc{Lucene} does not provide a parallel
one\footnote{Additionally, we did not feel that writing a correct and threadsafe
indexer as a user is something to be reasonably expected.}. \meta/ creates the
smallest index for gov2 while \textsc{Lucene} creates the smallest index for
blog06; \textsc{Indri} greatly lags behind both. \meta/ follows \textsc{Lucene}
closely in retrieval speed, with \textsc{Indri} again lagging. As expected,
query performance between the three systems is relatively even, and we attribute
any small difference in MAP or precision to idiosyncrasies during tokenization.

\input{ir-datasets}
\input{ir-indexing}
\input{ir-index-size}
\input{ir-query-speed}
\input{ir-map}

\subsection{Machine Learning}

\meta/'s ML performance is compared with \textsc{liblinear}~\cite{liblinear}, a
well-known SVM library; \textsc{scikit-learn}~\cite{scikit} a Python ML library;
and \textsc{SVMMulticlass}~\cite{svmmulticlass}, a competitor to
\textsc{liblinear}. We focus on linear classification across these tools.
Statistics for the five datasets used can be found in
Table~\ref{table:ml-datasets}.

Look at MALLET~\cite{mallet}.

Four of the datasets
(20news\footnote{\url{http://qwone.com/~jason/20Newsgroups/}}, Newegg, and the
Yelp Academic Dataset\footnote{\url{https://www.yelp.com/academic_dataset}}) are
textual datasets. The Newegg (crawled ourselves) and Yelp datasets are review
datasets, and we consider only a partial list of the whole dataset where we have
removed all three-star reviews. We used MeTA for tokenization and feature
generation, applying the same constraints as we did for the indexing tests.
Randomized training (two thirds) and test splits (one third) were generated for
each of these datasets. For rcv1, we used the existing tokenization and
training/test splits available on the LIBSVM data
website~\footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/}}.

In Table~\ref{table:ml-exp}, we can see that \meta/ performs well both in
terms of speed and accuracy, and presents itself as a viable option in the
machine learning domain.

\input{ml-datasets}
\input{ml-exp}
