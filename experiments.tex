\section{Experiments}
\label{sec:experiments}

To demonstrate \meta/'s effectiveness in three crucial fields, we evaluate its
performance in NLP, IR, and ML tasks. All experiments were performed on a
workstation with an Intel(R) Core(TM) i7-5820K CPU, 16 GB of RAM, and a 4
TB 5900 RPM disk.

\subsection{Natural Language Processing}

Talk about parsers and POS taggers.

Results in Table~\ref{table:nlp-pos}.
Results in Table~\ref{table:nlp-shallow}.

\input{nlp-datasets}
\input{nlp-pos}
\input{nlp-shallow}

\subsection{Information Retrieval}

\meta/'s IR performance is compared with two well-known search engine toolkits:
\textsc{Lucene}'s latest version 5.5.0\footnote{\url{http://lucene.apache.org/}} and
\textsc{Indri}'s version 5.9\footnote{Indri 5.10 does not provide source
    code packages and thus could not be used. It is also known as
\textsc{Lemur}.}~\cite{lemur}.

For the IR experiments, we use the TREC blog06~\cite{blog06} permalink documents
and TREC gov2 corpus~\cite{gov2} To ensure a more uniform indexing environment,
all HTML is cleaned, terms are lower-cased, stop words are removed from a common
list of 431 stop words, Porter2 (\meta/) or Porter (Indri, Lucene) stemming is
performed, a maximum word length of 32 characters is set, and the original
documents are not stored in the index. In addition, each corpus is converted
into a single file with one document per line to reduce the effects of many file
operations.

We compare the following: indexing speed (Table~\ref{table:ir-indexing}), index
size (Table~\ref{table:ir-index-size}), query speed
(Table~\ref{table:ir-query-speed}), and query accuracy
(Table~\ref{table:ir-map}). The queries we use are the standard TREC topics
associated with each dataset.

\input{ir-datasets}
\input{ir-indexing}
\input{ir-index-size}
\input{ir-query-speed}
\input{ir-map}

\subsection{Machine Learning}

\meta/'s ML performance is compared with \textsc{liblinear}~\cite{liblinear}, a
well-known SVM library; \textsc{scikit-learn}~\cite{scikit} a Python ML library;
and \textsc{SVMMulticlass}~\cite{svmmulticlass}, a competitor to
\textsc{liblinear}. We focus on linear classification across these tools.
Statistics for the five datasets used can be found in
Table~\ref{table:ml-datasets}.

Four of the datasets
(20news\footnote{\url{http://qwone.com/~jason/20Newsgroups/}}, Newegg, and the
Yelp Academic Dataset\footnote{\url{https://www.yelp.com/academic_dataset}}) are
textual datasets. The Newegg (crawled ourselves) and Yelp datasets are review
datasets, and we consider only a partial list of the whole dataset where we have
removed all three-star reviews. We used MeTA for tokenization and feature
generation, applying the same constraints as we did for the indexing tests.
Randomized training (two thirds) and test splits (one third) were generated for
each of these datasets. For rcv1, we used the existing tokenization and
training/test splits available on the LIBSVM data
website~\footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/}}.

In Table~\ref{table:ml-exp}, we can see that \meta/ performs well both in
terms of speed and accuracy, and presents itself as a viable option in the
machine learning domain.

\input{ml-datasets}
\input{ml-exp}
