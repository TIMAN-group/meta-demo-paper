\section{Experiments}
\label{sec:experiments}

To demonstrate \meta/'s effectiveness in three crucial fields, we evaluate its
performance in NLP, IR, and ML tasks.

\subsection{Natural Language Processing}

Talk about parsers and POS taggers.

Results in Table~\ref{table:nlp-pos}.
Results in Table~\ref{table:nlp-shallow}.

\input{nlp-datasets}
\input{nlp-pos}
\input{nlp-shallow}

\subsection{Information Retrieval}

\meta/'s IR performance is compared with two well-known search engine toolkits:
\textsc{Lucene}'s latest version 5.5.0\footnotemark[12], a top-level Apache
venture; and \textsc{Indri}'s latest version 5.10~\cite{lemur}, a joint
collaboration between the University of Massachusetts Amherst and Carnegie
Mellon University. Note that \textsc{Indri} is also known as \textsc{Lemur}; in
this paper, we choose to use the former for consistency.

\footnotetext[12]{\url{http://lucene.apache.org/}}

For the IR experiments, we use the TREC blog06\footnotemark[7] permalink
documents and TREC gov2 corpus\footnotemark[8].
To ensure a more uniform indexing environment, all HTML is cleaned, terms are
lower-cased, stop words are removed from a common list of 431 stop words,
Porter2 (\meta/) or Porter (Indri, Lucene) stemming is performed, a maximum word
length of 32 characters is set, and the original documents are not stored in the
index. In addition, each corpus is converted into a single file with one
document per line to reduce the effects of many file operations.

\footnotetext[7]{\url{http://ir.dcs.gla.ac.uk/test_collections/blogs06info.html}}
\footnotetext[8]{\url{http://ir.dcs.gla.ac.uk/test_collections/gov2-summary.htm}}

Further, all IR experiments were run on a personal laptop with an Intel quad
core i7-2760QM (2.40GHz) CPU, eight gigabytes of memory, and a 7200 RPM disk.

We compare the following: indexing speed (Table~\ref{table:ir-indexing}), index
size (Table~\ref{table:ir-index-size}), query speed
(Table~\ref{table:ir-query-speed}), and query accuracy
(Table~\ref{table:ir-map}). The queries we use are the standard TREC topics
associated with each dataset.

\input{ir-datasets}
\input{ir-index-size}
\input{ir-indexing}
\input{ir-query-speed}
\input{ir-map}

\subsection{Machine Learning}

\meta/'s ML performance is compared with \textsc{liblinear}~\cite{liblinear}, a
well-known SVM library; \textsc{scikit-learn}~\cite{scikit} a Python ML library;
and \textsc{SVMMulticlass}~\cite{svmmulticlass}, a competitor to
\textsc{liblinear}. Statistics for datasets used in both parts can be found in
Table~\ref{table:ml-datasets}.

For the machine learning evaluation, we focus on linear classification across
toolkits. Many applications in \meta/ are in a textual domain, and linear
classification lends itself to the high-dimensional space that comes with text
documents. All experiments are performed on a system with a dual core Intel Core
i5 CPU (M460) clocked at 2.53GHz and eight gigabytes of RAM.

Four datasets (20news\footnotemark[6], Newegg, and the Yelp Academic
Dataset\footnotemark[10]) are textual datasets. The Newegg (crawled ourselves)
and Yelp datasets are review datasets, and we consider only a partial list of
the whole dataset where we have removed all three-star reviews. We used MeTA for
tokenization and feature generation, applying the same constraints as we did for
the indexing tests. Randomized training (two thirds) and test splits (one third)
were generated for each of these datasets. For rcv1, we used the existing
tokenization and training/test splits available on the LIBSVM data
website~\footnotemark[11].

\footnotetext[6]{\url{http://qwone.com/~jason/20Newsgroups/}}
\footnotetext[10]{\url{https://www.yelp.com/academic_dataset}}
\footnotetext[11]{\url{http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/}}

In Table~\ref{table:ml-exp}, we can see that \meta/ performs well both in
terms of speed and accuracy, and presents itself as a viable option in the
machine learning domain.

\input{ml-datasets}
\input{ml-exp}
