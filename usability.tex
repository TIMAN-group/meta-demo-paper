\section{Usability}

Consistency across components is a key feature that allows \meta/ to work
well with large datasets. This is accomplished via a three-layer architecture.
On the first layer, we have tokenizers, analyzers, and all the text processing
that accompanies them. Once a document representation is determined, this tool
chain is run on a corpus. The indexes are the second layer; they provide an
efficient format for storing processed data. The third layer---the application
layer---interfaces solely with indexes. This means that we may use the same
index for running an SVM as we do to evaluate a ranking function, \emph{without
processing the data again}.

Since all applications use these indexes, \meta/ is able to support out-of-core
classification with some classifiers. We ran our large classification dataset
that doesn't fit in memory---Webspam~\citep{Webb:2006:CEAS}---using the
\texttt{sgd} classifier. Where \textsc{liblinear} failed to run, \meta/ was able
to finish the classification in a few minutes.

Besides using \meta/'s rich built-in feature generation, it is possible to
directly use \textsc{libsvm}-formatted data. This allows preprocessed datasets
to be run under \meta/'s algorithms. Additionally, \meta/'s
\texttt{forward\_index} (used for classification), is easily convertible to
\textsc{libsvm} format. Thus the reverse is also true: you may do feature
generation with \meta/, and use it to generate input for any other program that
supports \textsc{libsvm} format.

\meta/ is hosted publicly on
GitHub\footnote{\url{https://github.com/meta-toolkit/meta/}}, which provides the
project with community involvement through its bug/issue tracker and fork/pull
request model. Its API is heavily
documented\footnote{\url{https://meta-toolkit.org/doxygen/namespaces.html}},
allowing the creation of Web-based applications (listed in
section~\ref{sec:framework}). The project website contains several tutorials
that cover the major aspects of the
toolkit\footnote{\url{https://meta-toolkit.org/}} to enable users to get started
as fast as possible with little friction. Additionally, a public
forum\footnote{\url{https://forum.meta-toolkit.org/}} is accessible for all
users to view and participate in user support topics, community-written
documentation, and developer discussions.

A major design point in \meta/ is to allow for most of the functionality to be
configured via a configuration file. This enables minimal effort exploratory
data analysis without having to write (or recompile) any code. Designing the
code in this way also encourages the components of the system to be pluggable:
the entire indexing process, for example, consists of several modular layers
which can be controlled by the configuration file.

An example snippet of a config file is given below; this creates a bigram
part-of-speech analyzer. Multiple \texttt{[[analyzers]]} sections may be added,
which \meta/ automatically combines while processing input.

{\small
\begin{verbatim}
   [[analyzers]]
   method = "ngram-pos"
   ngram = 2
   filter = [{type = "icu-tokenizer"},
             {type = "ptb-normalizer"}]
   crf-prefix = "crf/model/folder"
\end{verbatim}}

A simple class hierarchy allows users to add filters, analyzers, ranking
functions, and classifiers with full integration to the toolkit
(\emph{e.g.}\ one
may specify user-defined classes in the config file). The process for adding
these is detailed in the \meta/ online tutorials.

This low barrier of entry and ease in experiment setup has led \meta/ to be used
in text mining and analysis MOOCs reaching over 40,000
students\footnote{\url{https://www.coursera.org/course/textretrieval}}$^,$\footnote{\url{https://www.coursera.org/course/textanalytics}}.

Multi-language support is hard to do correctly. Many toolkits sidestep this
issue by only supporting ASCII text or the OS language; \meta/ supports multiple
(non-romance) languages by default, using the industry standard ICU
library\footnote{\url{http://site.icu-project.org/}}. This allows \meta/ to
tokenize arbitrarily-encoded text in many languages.

Unit tests ensure that contributors are confident that their modifications do
not break the toolkit. Unit tests are automatically run after each commit and
pull request, so developers immediately know if there is an issue (of course,
unit tests may be run manually before committing). The unit tests are run in a
continuous integration setup where \meta/ is compiled and run on Linux, Mac OS
X\footnote{\url{https://travis-ci.org/meta-toolkit/meta}}, and
Windows\footnote{\url{https://ci.appveyor.com/project/skystrife/meta}} under a
variety of compilers and software development configurations.
