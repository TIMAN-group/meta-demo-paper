\section{Built for Big Data}

Consistency across components is a key feature that allows \meta/ to work
well with large datasets. This is accomplished via a three-layer architecture.
On the first layer, we have tokenizers, analyzers, and all the text processing
that accompanies them. Once a document representation is determined, this tool
chain is run on a corpus. The indexes are the second layer; they provide an
efficient format for storing processed data. The third layer---the application
layer---interfaces solely with indexes. This means that we may use the same
index for running an SVM as we do to evaluate a ranking function, \emph{without
processing the data again}.

Since all applications use these indexes, \meta/ is able to support
out-of-core classification with some classifiers. We ran our large
classification dataset, rcv1~\cite{rcv1}, with limited memory (only 95MB) using
the \texttt{sgd} classifier. Where \textsc{liblinear} failed to run,
\meta/ was able to finish the classification in just under two minutes.

Besides using \meta/'s rich built-in feature generation, it is possible to
directly use \textsc{libsvm}-formatted data. This allows preprocessed datasets
to be run under \meta/'s algorithms. Additionally, \meta/'s
\texttt{forward\_index} (used for classification), is stored as \textsc{libsvm}
format. Thus the reverse is also true: you may do feature generation with
\meta/, and use its index as input to any other program that supports
\textsc{libsvm} format.
