\section{A Unified Framework}
\label{sec:framework}

As NLP techniques become more and more mature, we have great opportunities to
use them to develop and support many applications, such as search engines,
classifiers, and integrative applications that involve multiple components. It's
possible to develop each application from scratch, but it's much more efficient
to have a general toolkit that supports multiple application types.

Existing tools tend to specialize on one particular area, and as such there is a
wide variety of tools one must sample when performing different data science
tasks. For text-mining tasks, this is even more apparent; it is extremely
difficult (if not impossible) to find tools that support both traditional
information retrieval tasks (like tokenization, indexing, and search) alongside
traditional machine learning tasks (like document classification, regression,
and topic modeling).

Table~\ref{tab:feature-comp} compares \meta/'s many features across various
dimensions. Note that only \meta/ % can find the peers
satisfies all the areas while other toolkits focus on a particular area. In the
case where the desired functionality is scattered, data science students,
researchers, and practitioners must find the appropriate software packages for
their needs and compile and configure each appropriate tool. Then, there is the
problem of data formatting---it is unlikely that the tools all have standardized
upon a single input format, so a certain amount of ``data munging'' is required.
All of this detracts from the actual task at hand, which has a marked impact on
productivity.

The goal of the \meta/ project is to address these issues. In particular, we
provide a unifying framework for existing machine learning and natural language
processing algorithms, allowing researchers to quickly run controlled
experiments. We have modularized the feature generation, instance
representation, data storage formats, and algorithm implementations; this allows
users to make seamless transitions along any of these dimensions with minimal
effort. Finally, \meta/ is dual-licensed under the University of Illinois/NCSA
Open Source Licence and the MIT License to reach the broadest audience possible.

\begin{table*}[t]
    \begin{center}
    {\small
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
        \hline
        & \textbf{Indri} & \textbf{Lucene} & \textbf{MALLET} &
        \textbf{LIBLINEAR} & \textbf{SVM$^{MULT}$} & \textbf{scikit} &
        \textbf{CoreNLP} & \textbf{\meta/} \\
        & \emph{IR} & \emph{IR} & \emph{ML/NLP} & \emph{ML} & \emph{ML} &
        \emph{ML/NLP} & \emph{ML/NLP} & \emph{all} \\
        \hline
        Feature generation & \checkmark & \checkmark & \checkmark & & &
        \checkmark & \checkmark & \checkmark \\
        Search & \checkmark & \checkmark & & & & & & \checkmark \\
        Classification & & & \checkmark & \checkmark & \checkmark & \checkmark &
        \checkmark & \checkmark \\
        Regression & & & \checkmark & \checkmark & \checkmark & \checkmark &
        \checkmark & \checkmark \\
        POS tagging & & & \checkmark & & & & \checkmark & \checkmark \\
        Parsing & & & & & & & \checkmark & \checkmark \\
        Topic models & & & \checkmark & & & \checkmark & & \checkmark \\
        $n$-gram LM & & & & & & & & \checkmark \\
        Word embeddings & & & \checkmark & & & & \checkmark & \checkmark \\
        Graph algorithms & & & & & & & & \checkmark \\
        Multithreading & & \checkmark & \checkmark & & & \checkmark & \checkmark
        & \checkmark \\
        \hline
    \end{tabular}
    \caption{Toolkit feature comparison. Citations for all toolkits may be found
        in their respective comparison sections.}
    \label{tab:feature-comp}
    }
    \end{center}
\end{table*}

Due to space constraints, in this paper, we only delve into \meta/'s natural
language processing (NLP), information retrieval (IR), and machine learning (ML)
components in section~\ref{sec:experiments}. We briefly outline all components
here:

\textbf{Feature generation}.
\meta/ has a collection of tokenizers, filters, and analyzers that convert raw
text into a feature representation. Basic features are $n$-gram words, but other
analyzers make use of different parts of the toolkit, such as POS tag $n$-grams
and parse tree features. An arbitrary number of feature representations may be
combined; for example, a document could be represented as unigram words, bigram
POS tags, and parse tree rewrite rules. Users can easily add their own feature
types as well, such as sentence length distribution in a document.

\textbf{Search}.
The \meta/ search engine can store document feature vectors in an inverted index
and score them with respect to a query. Rankers include vector space models such
as Okapi BM25~\citep{bm25} and probabilistic models like Dirichlet prior
smoothing~\citep{zhai-lm}. A search demo is
online\footnote{\url{https://meta-toolkit.org/search-demo.html}}.

\textbf{Classification}.
\meta/ includes a stochastic gradient descent (SGD) implementation with
pluggable loss functions, allowing creation of an SVM classifier (among others).
All-\emph{vs}-all and one-\emph{vs}-all ensemble methods for binary classifiers
allow multiclass classification. Other classifiers like na{\"i}ve Bayes and
$k$-nearest neighbors also exist. A confusion matrix class and significance
testing framework allow evaluation and comparison of different methods and
feature representations.

\textbf{Regression}.
Regression via SGD predicts real-valued responses from featurized documents.
Evaluation metrics such as mean squared error and $R^2$ score allow model
comparison.

\textbf{POS tagging}.
\meta/ contains a linear-chain conditional random field for POS tagging and
chunking applications, learned using $\ell_2$ regularized SGD~\citep{crf}. It
also contains an efficient greedy averaged perceptron tagger~\citep{greedy}.

\textbf{Parsing}.
A fast shift-reduce constituency parser using generalized averaged perceptron
~\citep{const-parsing} is \meta/'s grammatical parser. Parse tree featurizers
implement different types of structural tree representations~\citep{structural}.
An NLP demo online presents tokenization, POS-tagging, and
parsing\footnote{\url{https://meta-toolkit.org/nlp-demo.html}}.

\textbf{Topic models}.
\meta/ can learn topic models over any feature representation using collapsed
variational Bayes~\citep{cvb}, collapsed Gibbs sampling~\citep{gibbs}, stochastic
collapsed variational Bayes~\citep{scvb}, or approximate distributed
LDA~\citep{pargibbs}.

\textbf{$n$-gram language models} (LMs).
\meta/ takes an ARPA-formatted
input\footnote{\url{http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html}}
and creates a language model that can be queried for token sequence
probabilities or used in downstream applications like
SyntacticDiff~\citep{syndiff}.
% MPH not merged yet, mention?

\textbf{Word embeddings}.
The GloVe algorithm~\citep{glove} is implemented in a streaming framework and
also features an interactive semantic relationship demo. Word vectors can be
used in other applications as part of the \meta/ API\@.

\textbf{Graph algorithms}.
Directed and undirected graph implementations exist and various algorithms such
as betweenness centrality, PageRank, and myopic search are available. Random
graph generation models like Watts-Strogatz and preferential attachment exist.
For these algorithms see \citet{networks}.

\textbf{Multithreading}.
When possible, all \meta/ algorithms and applications are parallelized using C++
threads to make full use of available resources.
